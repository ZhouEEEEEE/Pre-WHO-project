---
title: "JSC-HW4"
author: "Shiyuan Zhou"
date: "2022/3/28"
output:
  pdf_document: default
  html_document: default
---

# Introduction

In this assignment, we have two parts. For the first part, we analyze high performance computing on functions and simulations. We may or may not apply parallel computation in each situation. Then, we compare time record to see how more efficient we make. In the second part, based on given dataset, we will fit regression tree, bagging, random forest, gradient boosting, and extreme gradient boosting models to investigate our research questions. Then, we compare their performance by MSE and pick the best model among them.

# HPC

```{r message=FALSE, warning=FALSE, include=FALSE}
Sys.setenv(LANGUAGE = "en")
```

## Problem 1

In this section, we can rewrite function 1 and 2 to be more efficient without using parallel computing.

```{r message=FALSE, warning=FALSE, include=FALSE}
library(parallel)
library(foreach)
library(doParallel)
```


```{r}
# Total row sums
fun1 <- function(mat) {
  n <- nrow(mat)
  ans <- double(n)
  for (i in 1:n) {
    ans[i] <- sum(mat[i, ])
  }
  ans
}

fun1alt <- function(mat) {
  rowSums(mat)
}

set.seed(2315)
dat <- matrix(rnorm(200 * 100), nrow = 200)

microbenchmark::microbenchmark(fun1(dat), fun1alt(dat), unit = "microseconds", 
                               check = "equivalent")
```

According to the table we have, we have save more than 100 microseconds if we use rowSums() in 'fun1alt', instead of looping over each row and get their sum in 'fun1'.

```{r}

# Cumulative sum by row
fun2 <- function(mat) {
  n <- nrow(mat)
  k <- ncol(mat)
  ans <- mat
  for (i in 1:n) {
    for (j in 2:k) {
      ans[i,j] <- mat[i, j] + ans[i, j - 1]
    }
  }
  ans
}

fun2alt <- function(mat) {
  t(apply(mat,1, cumsum))
}

# Test for the second
microbenchmark::microbenchmark(fun2(dat),fun2alt(dat), unit = "microseconds", 
                               check = "equivalent")

```

According to the table we have, we have save more than 1000 microseconds if we apply 'cumsum' function to our data 'mat' by apply() in fun2alt, instead of nested loop and summation in 'fun2'.

\newpage

## Parallel Computing Problem 2

In this section, we will use parallel computing to simulate PI and compare with our original method. We use system.time() to measure the execution runtime.


```{r}
sim_pi <- function(n = 1000, i = NULL) {
p <- matrix(runif(n*2), ncol = 2)
mean(rowSums(p^2) < 1) * 4
}

set.seed(1231)
system.time({
  ans <- unlist(lapply(1:4000, sim_pi, n = 10000))
  print(mean(ans))
})

# MY CODE HERE
system.time({
  # MY CODE HERE
  ncpus <- detectCores() # Detect number of cores we have
  cl <- makePSOCKcluster(ncpus)
  clusterSetRNGStream(cl, 1231)
  ans <- unlist(parLapply(cl, 1:4000, sim_pi, n = 10000))
  print(mean(ans))
  # MY CODE HERE
  stopCluster(cl)
})

```

According to the result we have, the simulated Pi of parallel computing is very close to that of our original method and even closer to true value, 3.14159. Column 'user' indicating CPU time spent by the current process, which is decreased by 2.83 seconds by parallel computing. Column 'system' indicating the CPU time spent by the kernel on behalf of the current process, which is almost the same.


\newpage

# ML

In this section, we will perform basic machine learning techniques on our data for 332 major league baseball players. We will fit regression tree, bagging, random forest, gradient boosting, and extreme gradient boosting models to predict players' salary based on the features in the data. Then, we will compare all the models based on MSE.

```{r message=FALSE, warning=FALSE, include=FALSE}
url <- "https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/hitters/hitters.csv"
dataset <- read.csv(url)
dataset <-na.omit(dataset)

set.seed(2001)
train_ind <- sample(1:nrow(dataset),round(0.7*nrow(dataset)))
train <- dataset[train_ind,]
test <- dataset[-train_ind,]
```

## Fit a regression tree and appropriately prune it based on the optimal complexity parameter

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(caret)
```

We fit the regression tree by the following code.

```{r echo=TRUE}
treefit <- rpart(Salary~., method = "anova", 
                 control = list(minsplit=10,minbucket=3,cp=0,xval=10),
                 data = train)
```

```{r echo=FALSE}
rpart.plot(treefit)
```


```{r echo=FALSE}
xe <- min(treefit$cptable[,"xerror"])
# printcp(treefit)
```
When xerroe is `r xe` at the 9th split, it is minimized. Hence, we could get the corresponding optimal complexity parameter and prune the regression tree.
 
```{r echo=TRUE}
optimalcp = treefit$cptable[which.min(treefit$cptable[,"xerror"]),"CP"]
treepruned <- prune(treefit, cp=optimalcp)
```

### Plotting the full pruned regression tree

```{r echo=FALSE}
rpart.plot(treepruned)
```


```{r echo=FALSE}
# MSE
tree_pred <- predict(treepruned,test)
test_tree <- cbind(test,tree_pred)
tree_mse <- mean((test_tree$tree_pred - test_tree$Salary)^2)

```

\newpage

## Predict Salary using bagging

We fit the bagging model by follwoing method with randomForest().

```{r echo=TRUE}
sal_bag<- randomForest(Salary~.,data=train,mtry=19,na.action=na.omit)
```

### Variable importance plot

```{r echo=FALSE}
# sum(sal_bag$err.rate[,1])
varImpPlot(sal_bag,n.var = 19,col="red")
# importance(sal_bag)
```

According to the variable importance plot we have for bagging model, variable 'CRBI' decreases node impurity most among all the variables. However, other variables are at least half significance as 'CRBI' and not very clear.


```{r echo=FALSE}
# bag_mse <- sal_bag$mse[length(sal_bag$mse)]
yhat.bag = predict(sal_bag, newdata = test)
bag_mse <- mean((yhat.bag-test$Salary)^2)
```


\newpage


## Predict Salary using random forest

We fit the random forest model by follwoing method with randomForest().

```{r echo=TRUE}
sal_rf <- randomForest(Salary~.,data=train,na.action = na.omit)
```

### Variable importance plot

```{r echo=FALSE}
# sum(sal_rf$err.rate[,1])
varImpPlot(sal_rf,n.var=19,col="blue")
# importance(sal_rf)
```

In the variable importance graph, variable 'CRBI' also have the strongest decrease on node impurity in random forest model. Comparing to bagging model, variables in random forest model is clearer and the importance is gradually decrease along different variables.

```{r echo=FALSE}
# rf_mse <- sal_rf$mse[length(sal_rf$mse)]
yhat.rf = predict(sal_rf, newdata = test)
rf_mse <- mean((yhat.rf-test$Salary)^2)
```

\newpage


## Perform Boosting on a range of values of shrinkage parameter

```{r echo=FALSE}
# url <- "https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/hitters/hitters.csv"
# dataset <- read.csv(url)
# set.seed(2496)
```

### Data Wrangling

Before we perform gradient boosting, we need to wrangle the data. We need to make character variables into numeric variables and get rid of missing values since boosting model cannot apply to categorical variables.

By looking at the dataset, we found that variable 'League', 'Division', and 'NewLeague' are categorical. Since they are binary, we only need to convert them into 1 and 0 and create new variables. Then, we remove the original variable and remove NAs.

```{r echo=TRUE}
dataset$League_num <- ifelse(dataset$League=="A",1,0)
dataset$Division_num <- ifelse(dataset$Division=="E",1,0)
dataset$NewLeague_num <- ifelse(dataset$NewLeague=="A",1,0)
dataset <- dataset %>% select(-c(League, Division, NewLeague))
```

```{r echo=FALSE}
set.seed(2496)
train = sample(1:nrow(dataset), floor(nrow(dataset) * 0.7))
test = setdiff(1:nrow(dataset), train)
```

### Fiiting Models and Plot Value of Shrinkage Parameter vs Training Set MSE

We tuned the shrinkage parameter by a for loop. For each iteration, we fit a gradient boosting model by gbm() and save the train error and cross validation error into 'mse' and 'cv'.

The range of shrinkage value we picked is from 0.001 to 0.025 by 0.0005 on each step. Since we only have 1000 trees, small shrinkage like 0.001 may be too slow and hard to decay to a optimal point. Too large learning rate, like more than 0.025 or even close to 1 may result in a very over-fitted model. Hence, we use this range to find a moderate value of shrinkage parameter.

```{r echo=TRUE, message=FALSE, warning=FALSE}
mse <- c()
cv <- c()
for (lr in seq(0.001, 0.025, 0.0005)){
  set.seed(2496)
  sal_boost = gbm(Salary~., data = dataset[train,], distribution = "gaussian", n.trees = 1000, 
                  shrinkage = lr, interaction.depth = 1, cv.folds = 10, class.stratify.cv = T)
  mse <- c(mse, sal_boost$train.error[1000])
  cv <- c(cv, sal_boost$cv.error[1000])}
```

Then, we plot a plot with value of shrinkage parameter vs training set MSE.

```{r echo=FALSE, fig.width=7, fig.height=3}
shrinkage_val <- seq(0.001, 0.025, 0.0005)
x <-  data.frame(sh = shrinkage_val,
                   mse = mse)
ggplot(x, aes(x = sh, y = mse)) +
  geom_point() +
  xlab("Shrinkage parameter") +
  ylab("Training MSE")

```

### Parameter Tuning

According to the plot, we could find that as the shrinkage increases, the training MSE decreases. The reason for that may be we fit the training set better and better when the shrinkage increases, which may lead to an over-fitting. Hence, we need to pick the optimal value of shrinkage parameter by their cross validation error.

Then, we add the cross validation for each shrinkage as a blue line in the graph we have above.

```{r echo=FALSE, fig.width=7, fig.height=3}
shrinkage_val <- seq(0.001, 0.025, 0.0005)
x <-  data.frame(sh = shrinkage_val,
                   mse = mse)
ggplot(x, aes(x = sh, y = mse)) +
  geom_point() +
  xlab("Shrinkage parameter") +
  ylab("Training MSE") +
  geom_line(aes(y = cv), color = 'blue')

```

Pick 0.015 as our shrinkage parameter since the variation of cross validation error is flatten when parameter is greater than 0.015, which means higher shrinkage may not reduce validation error. The other reason that we don't pick higher learning rate is that we need to reduce over-fitting.

### Plot the Cross-validation Errors as a Function of the Boosting Iteration

```{r warning=FALSE, echo=FALSE, message=FALSE, fig.width=7, fig.height=4}
set.seed(2496)
sal_boost_6 = gbm(Salary~., data = dataset[train,], distribution = "gaussian", n.trees = 1000, shrinkage = 0.015, interaction.depth = 1, cv.folds = 10, class.stratify.cv = T)
plot(sal_boost_6$train.error)
lines(sal_boost_6$cv.error, col = 'blue')
# ylab("Train Error")
```

According to the plot we have, the deviation between validation error and train error is become smaller as we have more iterations.

```{r, message=FALSE, echo=FALSE, fig.width=7, fig.height=4}
# str(summary(sal_boost))
# summary(sal_boost_6)
# walk <- summary(sal_boost_6)$rel.inf[1]

knitr::kable(summary(sal_boost_6), caption = "Reletive influence for each variable in Gradient Boosting Model")
```

According to the variable importance plot we have, there is a clear difference in relative influence between variables. Additionally, we have the table for each variable and their corresponding relative influence in Gradient Boosting Model. The variable that is the most influential is 'CRBI' with relative influence 13.3311383

```{r echo=FALSE}
yhat_boost <- predict(sal_boost_6, newdata = dataset[test,], n.trees = 1000)
boost_mse <- mean((yhat_boost-dataset[test, "Salary"])^2)
```

\newpage

## Perform XGboost on a range of values of shrinkage parameter

Based on the wranglled data, we perform extreme gradient boosting model to predict salary. We setted up a tuning grid that can help us to perform grid search on eta, max_depth, and nrounds. Based on our data, and 'xgbTree' method, we train our xgb model on the tune grid.

```{r warning=FALSE, message=FALSE, echo=TRUE}
set.seed(2496)
train_control = trainControl(method = "cv", number = 10, search = "grid")

tune_grid <- expand.grid(max_depth = c(1,3,5,7),
                         nrounds = (1:10)*50,
                         eta = c(0.01, 0.1, 0.3),
                         gamma = 0,
                         subsample = 1,
                         min_child_weight = 1,
                         colsample_bytree = 0.6
                         
)
sal_xgb <- caret::train(Salary~., data=dataset[train,], 
                        method = "xgbTree", trControl = train_control, 
                        tuneGrid = tune_grid, verbosity = 0)

```

After training, we have our variable importance plot.

```{r echo=FALSE, fig.width=8, fig.height=4}
plot(varImp(sal_xgb, scale = F))

yhat_xgb <- predict(sal_xgb, newdata = dataset[test,])
xgb_mse <- mean((yhat_xgb - dataset[test, "Salary"])^2)
```

According to the plot, we can see that the difference of importance between variables are pretty clear. Variable 'CRBI' is the most important feature in extreme gradient boosting model.

\newpage

## Comparing model by Test MSEs

After training and fitting 5 models: regression tree, bagging, random forest, gradient boosting, and extreme gradient boosting. We evaluate their performance based on their test MSEs. I calculated their test MSEs and collect them into the following table.

```{r echo=FALSE, warning=FALSE, message=FALSE}
dfsp <- data.frame(models = c("Pruned Regression Tree",
                              "Bagging",
                              "Random Forest",
                              "Gradient Boosting",
                              "Extreme Gradient Boosting"),
                   MSE = c(tree_mse, bag_mse, rf_mse, boost_mse, xgb_mse))

knitr::kable(dfsp, caption = "Comparing MSE of all models")
```

According to the MSE table, we can see that Bagging model has the smallest test MSE, which is 80766.93 Regression tree model has the largest MSE 166536.98, indicating a worse fit. Low test MSE shows high performance and low over-fitting. Hence, we may pick gradient boosting model as our final model to predict salary.



